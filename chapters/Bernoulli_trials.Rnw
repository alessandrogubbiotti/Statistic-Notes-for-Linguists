
	\section{ Bernoulli trials, Independent Samples and Beyond }
	 \label{s:bernoulli_and_beyond}

	 In Section~\ref{s:joint} we have seen what is needed to compute the joint probability of two experments, namely the probability of the first experiment and the probability of the second experment given the outcome of the first one, see \eqref{}. In the present Section we apply the theory developed in Section~\ref{} to strings $\underline X = X_1\ldots X_n$, for some $n \in \mathbb N$,  whose digits $X_i$ can be either 0 or 1, such as coin tosses. The collection of all possible strings is given by the sample space \eqref{e:}, and our aim will be to compute the probability of the events 
	\bel{e:event}{
	(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)
	}
	for some $n \in\mathbb N $ and $x_1, \ldots, x_n \in \{0,1\}$, see \eqref{e} for the notation that I am using and \eqref{e} at the corresponding event seen as a subset of $\Omega$. The two probability meausures we introduce are the Bernoulli meausre and Markov measures. Both cases are a particular case of \eqref{e} and \eqref{e:}, respectively, but, despite that we will rewrite the computations in this more concrete setting. As an historical note, Markov measures have bee  

 
\subsection{The Bernoulli measure}
	\label{ss:Bernoulli}
	\paragraph{ Definition }
	The Bernoulli measure is the measure induced by independent coint tosses of parameter $p$. A string of 0s and 1s cna be obtained from them  
	The sample space of $n \in \mathbb N $ coin tosses is $\Omega = \{x_1 \ldots x_n, x_i \in \{0,1\}\}$, and, using the same notations as in Example \ref{}, we can define the random variables that look at the result of each coin toss by
We use slightly different notations 
\bel{d:x1}{
		 X_1 = \begin{cases}
			 1 &  \textrm{ if the first toss is heads}\\
		 	0 & \textrm{ otherwise }\\
		 \end{cases}}

	 \bel{d:x2}{
		 X_2 = \begin{cases}
			 1 &  \textrm{ if the second toss is heads}\\
		 	0 & \textrm{ otherwise }
		 \end{cases}}

\bel{d:xn}{
		 X_n = \begin{cases}
			 1 &  \textrm{ if the n-th toss is heads}\\
		 	0 & \textrm{ otherwise }
		 \end{cases}}
	For a fixed word $x_1 \ldots x_n$ of $n$ digits with values in $0,1$, that is, for each possible outcome of the coin toss, 
	\bel{}{
		\{x_1\dots x_n\} = ( X_1 = x_1, X_2  = x_2,\ldots X_n = x_n)
	}
	As an example, $\{100\} = (X_1 = 1, X_2 = 0 , X_3 = 0 )$. Using the definition of conditional probability \eqref{d:cond}????, we can write, for each $x_1\ldots x_n \in \Omega$, 

	The above has the intuituive meaning, see also Example~\ref{d:}???? that to evaluate the probability of all the $n$ coin tosses we first evaluate the first probability, then thesecond one, but we must take track of the fact that we know the first result and so on, untill we evaluate the last coin toss, with the knowledge of all the previous tosses. Using the hypothesis of independence we have that 	

	\bel{e:indep_n}{
	& \mathbb{P}(X_2=x_2|X_1=x_1)=\mathbb{P}(X_2=x_2)\\
	 & \mathbb{P}(X_3=x_3|X_2 = x_2, X_1=x_1)=\mathbb{P}(X_3=x_3)\\
	 & ... \\
	 & \mathbb{P}(X_n=x_n|X_2=x_2,....,X_{n-1}=x_{n-1})=\mathbb{P}(X_n=x_n).
	}
	
	and, therefore, substituting \eqref{e:indep_n} into \eqref{e:bernoullin}, 
	
	For the sake of clarity, the above computations for $n =3$ and $x_1x_2 x_3 = 100$ read as 
	\bel{}{
		\mathbb P(\{100\}) &= \mathbb P(X_1 = 1 ,X_2 = 0 ,  X_3 = 0 ) = \mathbb P(X_1 =  1) \mathbb P(X_2 = 0  | X_1 = 1 ) \mathbb P(X_3 = 0 |X_2 =  0 ,  X_1 = 1  )\\
		& \mathbb P(X_1 =  1 ) \mathbb P(X_2 = 0 ) \mathbb P(X_3 =0 ) = p (1-p)(1-p) = p (1-p)^2
	} 
	
	\begin{definition}[ Bernoulli probability measure]
	\label{d:Bernoulli_measure}
	The Bernoulli measure of parameter $p \in [0,1]$ is a probability measure $\mathbb P_p$ that assigns to the event $A_{x_1\ldots x_n}$, for each $n \in \mathbb N$ and $x_1, \ldots, x_n \in \{0,1\}$, the  the probability 
	\bel{e:Bernoulli_p}{
		\mathbb P_p(A_{x_1\ldots x_n}) = p^{x_1 + \ldots + x_n}(1-p)^{n - (x_1 + \ldots x_n)}
	} 
	\end{definition}

	\begin{definition}[$n$ Bernoulli trials]
		The probability model given by the Bernoulli trials consists on the sample space $\Omega = \{x_1\ldots x_n , x_i \in \{0,1\}, \text{ for $ i = 1, \ldots, n$}\}$ endowed with the probability measure defined on the elementary events (see Deinition~\ref{d:elementary}) by  
	\bel{}{
		\mathbb P(\{x_1\ldots x_n\}) = p^{x_1 + \ldots + x_n}(1-p)^{n - (x_1 + \ldots + x_n)}
		}
	\end{definition}

	\paragraph{Simulation}
	\paragraph{Examples}
	Of course, Bernoulli trials might not only refer to coin tosses but also to any independent experiments with only two possible outcomes. The next example gives a sequence
	\begin{example}[ Product Manifacturing Errors]
	An applicaiton
	\end{example}
%%% So, here I would like to put hte measure 
	\subsection{Other Kind of Dependence: Markov Chains}
		\label{ss:Markov}
		The Markov measure is obtained in the same way as \eqref{e:}
	\bel{}{
		\mathbb P(X_1 = x_1, \ldots, X_n = x_m)
	}
	In many contexts the independence is a very stron assumption to be made on the data. We will see in the next Subsection~\ref{e:Onegin} that if we take a text and we transoform each consonant into a $1$ and each vowel into a 0, the sequence thus obtained does not come from  Bernoulli trials. As another example, let $1$ denote a sunny dday and 0 a day where it rain. Then the Be
	
	\section{ Eugene Onegin}
	\paragraph{Definition}
	Having introduced the statistical framework in Subsection\ref{ss:models} what models are, we are ready to see more in detail the Eugene Onegin example introduced in Example~\ref{e:Eugene_Onegin1}.  

One can think that the independence hypothesis is too strict and would like to weaken it. However, one does not want to consider the event 
	
depending on all the $2^{n-1}$ possible values of $X_1$,...,$X_{n-1}$. One hypothesis one usually makes is the Markov hypothesis, which is to say that the variable $X_n$ depends only on the variable $X_{n-1}$, so that one obtains from \eqref{} the following

The Markov Chains have become a huge topic in mathematics, as they are a non-trivial example of dependence in which you can make computations with them (for instance, the Law of Large numbers holds). However they have a linguistic Origin, since Markov introduced them in the context of vowels and consonant instances in the Pushkin's poem Eugene Onegin. If you consider $X_i = 1$ if the i-th letter of the poem is a consonant and 0 otherwise, he showed that the probability given by the Bernoulli trials for the sequence 10001011000... is not a good choice. For an historical introduction: 
	
	\paragraph{Simulation }
	
	We can simulate a Markov chain with state space $\{0,1\}$ easily following the reasonng in \eqref{e:}. We start from $X_1 = 0$ for simplicity, and then we take $X_1$ to be 1 with probability $a$ and 0with probability $1-a$. If $X_1 = 0$ then $X_2$ is chosen with the same procedure, otherwise, if $X_1 = 1$, $X_2 = 0$ with probability $b$ and $1$ with probability $1-b$, and so on. The general rule is that $X_i = 1$ with probability $b$ if the previous element is $X_{i-1} = 1$, and $X_i = 0 $ with probability $a$ if $X_{i-1} = 1$.        
<<>>= 
n <- 1000 #number of variables to simulate
a <- 0.7 #probability of jumping to 1 while at 0
b <- 0.2 #probability of jumping to 0 while at 1
simulate_markov_chain <- function(n, a, b, start_state = 0) {
 states <- numeric(n)
 states[1] <- start_state 
 for (i in 2:n) {
  if (states[i - 1] == 0) {
   states[i] <- rbinom(1, 1, a)  # 0 -> 1 with prob a
  } 
  else {
   states[i] <- rbinom(1, 1, 1 - b)  # 1 -> 1 with prob 1 - b
  }
 }
 return(states)
}
X <- simulate_Markov_chain(n,a,b)
X[1:30]
@

	\paragraph{ Example: Eugene Onegin }
	We will now describe a sequence $\underline X$ of 0s and 1s that looks more similar to an output of the above chuck of code than an output of the provious one simulating the i.i.d. variables. What does it mean to look like in this context is left ambiguous on purpose??, since it is a topic of statistics. While using proability theory we can define the machinary that generates data according to a measure that we fix, in this case the functions \textit{rbinom(n , size = 1, prob = p )} and \textit{ simulate\_markov\_chain}, statistics looks at the data and aims to decide what is the "machinary" that has generated the data. We will come back to the next example in the second part on the book, see Example~\ref{e:Eugene_Onegin2} and Example~\ref{e:Eugene_Onegin3}.     
	\begin{example}[Eugene Onegin]
		\label{ex:Eugene_Onegin1}
	The first application of Markov measures has been introduced by Markov, see \url{https://www.americanscientist.org/sites/americanscientist.org/files/201321152149545-2013-03Hayes.pdf} for an historical note. The sequence of 0s and 1s was obtained by replacing consonants by 1 and vowels by 0. We will do the same in the next chunck of code, but for all the dramas in the German drama corpus using the R package \textit{rdracor}, see \url{https://github.com/dracor-org/rdracor}.  
<<>>=
ger <- rdracor::get_dracor(corpus = 'ger') 
binary_transform_ger <- function(drama_name){
  text <- rdracor::get_text_chr_spoken(play = drama_name, corpus = "ger")
  all_text <- paste(text, collapse = "")
  letters_vector <- tolower(unlist(strsplit(all_text, "")))
  spoken_chars <- c(letters," ̈a", " ̈o", " ̈u", "ß")
  filtered_letters_vector <- letters_vector[ letters_vector %in% spoken_chars] # Keep only german_vowels <- c("a", "e", "i", "o", "u", " ̈a", " ̈o", " ̈u")
  binary_sequence <- ifelse(filtered_letters_vector %in% german_vowels, 0, 1) 
  return(binary_sequence)
}
text_01 <- vector(mode = 'numeric') 
for( play_name in ger$playName[1:10]){
 text_01 <- c(vec, binary_transform_ger(play_name)) 
}
vec[1:30]
@
	\end{example}
	\begin{ExerciseList}
		\Exercise Observe the sequence of 0s and 1s defined saved in the above chunck of code in the variable \textit{text\_01} and the one defined by \textit{rbinom(n, size = 1, prob = p)}. What differences do you notice 
	\end{ExerciseList}

	\subsection{Binomial coefficient}
\begin{example}[Three Bernoulli trials]

	In this case \bel{}{A_0 & = \{000\}\\
				A_1 & =\{001,010,100\}\\
				A_2 & =\{011,101,110\}\\
				A_3 & = \{111\}\\
				}

			So that $|A_0|=|A_3| = 1 $, $|A_1 |= |A_2| =2$.
\end{example} 



\begin{example}[4 Bernoulli trials]

	In this case \bel{}{A_0 & = \{0000\}\\
				A_1 & =\{0001,0010,0100,1000\}\\
				A_2 & =\{0011,0101,0110,1001,1010,1100\}\\
				A_3 & = \{0111,1011,1101,1110\}\\
				A_4 & = \{1111\}
				}

			So that $|A_0|=|A_3| = 1 $, $|A_1 |= |A_2| =2$.
\end{example} 
Note that there is a symmetry between $A_k$ and $A_{n-k}$. I can obtain $A_{n-k}$ simply by changing 0 with 1, that is, by changing the role of heads with the role of tails. In the general case, the problem consists in find the number of ways of writing a string of n digits with $k$ 1 and $n-k$ zeros. We can proceed as follow: 
\begin{itemize}
	\item We place the 1st 1, and we have $n$ possible places 
	\item We place the 3nd 1, and we have $n-1$ possibilities 
		... 
	\item We plavce the kth 1, and we have $n-k+1$ possibilities
\end{itemize}


We obtain $n(n-1)....(n-k+1)$ ways of counting of performing the above procedure. However you can notice that we are counting the same configuration a multiple times. Denote by $X_i$ the position of the ith 1. For $k=2$ and $n=4$, for example, $X_1 = 3$, $X_2 = 4$ give rise to thesame configuration as it gives $X_1 = 4 $ and $X_2 =3$, and this configuration is 0011.  For $k= 3$ and $n=5$, for example, we are counting as different the configurations $X_1 = 3, X_2=4, X_3=5$,  the configuration $X_1 =3 ,X_2= 5 , X_3 =4$, the configuration $X_1 = 5, X_2 = 4, X_3 =5 $,...., all corresponding to the elementary event  00111. You can convince yourself that the we are counting the above configuration $k!$ times. Thus the number of elements in $A_k$ is 
\bel{}{\frac{n(n-1)...(n-k+1)}{k(k-1)...1}= \frac{n!}{(n-k)!k!}.}
The above number has a name and is denoted the binomial coefficient $n\choose{k}$, and has the interpretation of being the number of ways you can choose k elements from a set of n distinct elements , disregarding the order 

\subsection{Binomial distribution}
Consider $3$ unfair Bernoulli trials with success probability $p$, and define the random variable $H= $ The number of heads. We know that $H$ can assume only 4 values: 0,1,2,3.  What is the probability distribution of $H$? We need to compute $\mathbb{P}(H=0)$,....,$\mathbb{P}(H = 3)$, and we have all the elements to do so. 
The event (H =0) corresponds to $A_0=\{000\}$ , the event $( H = 1) $ to $A_1 = \{001,010,100\}$, the event $(H =2 ) $ to $A_2=\{011,101,110\}$ and the event $(H = 3) $ to $A_3 =\{111\}$. 
recall that the probability measure is given by formula \eqref{e:bernoulli3} . 
\bel{}{\mathbb{P} ( H =0 ) = \mathbb{P}(000) = (1-p)^3}
\bel{}{\mathbb{P}(H = 1) & = \mathbb{P}(001)+ \mathbb{P}(010)+\mathbb{P}(100) = p(1-p)^2+ p(1-p)^2+ p(1-p)^2= 3p(1-p)^2}
\bel{}{\mathbb{P}(H = 2) & = \mathbb{P}(011)+ \mathbb{P}(101)+\mathbb{P}(110) = p^2(1-p)+ p^2(1-p)+ p^2(1-p)= 3p^2(1-p)}
\bel{}{\mathbb{P} ( H =3 ) = \mathbb{P}(111) = p^3}
Note that the elements in $A_i$ have all the same probability, equal to $p^i(1-p)^{3-i}$. How many time are we summing those numbers to compute $\mathbb{P}(A_i)$? Exactly the number of elements of $A_i$, which we have seen to be equal to ${3\choose i}$. \\
Consider now $n$ bernoulli trials, that is, consider the probability measure \eqref{e:bernoullin} on $\Omega_n$. Consider the variable $H$ equal to the number of heads. The event $ (H = k) $ is the event $A_k = \{x_1...x_n\in \Omega_n\, x_1+....+x_n = k\}$. By definition of \eqref{e:bernoullin} we see that  if $x_1....x_n\in A_k $ then $\mathbb{P}(x_1...x_n)= p^k(1-p)^{n-k}$ . Thus 
\bel{}{\mathbb{P}(H = k) =\sum_{x_1x_2...x_n\in A_k} \mathbb{P}(x_1....x_n) =\mathbb{P}(00...11)+...+ \mathbb{P}(111...00).}
By the above observations we are summing $p^k(1-p)^{n-k}$ a number of times equal to the numer of elements of $A_k: {n\choose k}$. Thus
\bel{}{\mathbb{P}( H = k ) = {n\choose k} p^k(1-p)^{n-k}.}
\begin{definition}[Binomial Random Variable ]
	A random variable $X$ whose possible values are 0,1,...,n and whose distribution is given by 
	\bel{}{\mathbb{P}(X =0) (1-p)^n\,\,\mathbb{P}( X = 1) = n p (1-p)^{n-1}\,...\, \mathbb{P} ( X = i) = {i\choose n}p^i(1-p)^{n-i}\,....\,\mathbb{P}( X =n ) = p^n,}
	is said to be a binomial random variable of parameter $n$ and $p$. 
\end{definition}
In this section we have seen that the $H_k$ are examples binomial random variables of parameter $n$ and $p$. 

	\subsection{An Example of not Coming from Bernoulli trials}
	
